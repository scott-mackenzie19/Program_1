{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cxcTHUiY5QHE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "from gensim import corpora, models\n",
        "from gensim.models import CoherenceModel\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required data for text processing\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ0PpJQF5Zoq",
        "outputId": "ebe81b03-3a45-43ae-88cd-3408fd93e51d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for consistent results\n",
        "def initialize_seed(seed_value=123):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "initialize_seed()"
      ],
      "metadata": {
        "id": "PmFUucnK5dXX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing function\n",
        "def clean_text(input_text):\n",
        "    \"\"\"\n",
        "    Cleans text by removing non-alphabet characters, converting to lowercase,\n",
        "    tokenizing, and filtering stopwords.\n",
        "\n",
        "    Args:\n",
        "        input_text (str): The text to process.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of processed words.\n",
        "    \"\"\"\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z]', ' ', input_text)\n",
        "    tokens = word_tokenize(cleaned_text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "    return filtered_tokens\n"
      ],
      "metadata": {
        "id": "MFvvQoeX5fPV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class for IMDB reviews\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, reviews, sentiments, tokenizer, max_len=512):\n",
        "        self.reviews = reviews\n",
        "        self.sentiments = sentiments\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        tokens = self.tokenizer(\n",
        "            self.reviews[index],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        tokens = {key: val.squeeze() for key, val in tokens.items()}\n",
        "        tokens['labels'] = torch.tensor(self.sentiments[index], dtype=torch.long)\n",
        "        return tokens"
      ],
      "metadata": {
        "id": "9QoaqoDr5hRs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and split IMDB dataset\n",
        "def prepare_review_data(file_path, tokenizer, split_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Loads IMDB reviews and splits them into training and testing sets.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the dataset file.\n",
        "        tokenizer (BertTokenizer): Tokenizer for processing reviews.\n",
        "        split_ratio (float): Proportion of data for testing.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[ReviewDataset, ReviewDataset]: Training and testing datasets.\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(file_path)\n",
        "    sentiment_map = {'positive': 1, 'negative': 0}\n",
        "    data['sentiment_label'] = data['sentiment'].map(sentiment_map)\n",
        "    reviews = data['review'].tolist()\n",
        "    sentiments = data['sentiment_label'].tolist()\n",
        "\n",
        "    train_reviews, test_reviews, train_sentiments, test_sentiments = train_test_split(\n",
        "        reviews, sentiments, test_size=split_ratio, stratify=sentiments, random_state=42\n",
        "    )\n",
        "\n",
        "    train_data = ReviewDataset(train_reviews, train_sentiments, tokenizer)\n",
        "    test_data = ReviewDataset(test_reviews, test_sentiments, tokenizer)\n",
        "\n",
        "    return train_data, test_data"
      ],
      "metadata": {
        "id": "Uj8gT4oZ5nT6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fine-tune a BERT model\n",
        "def train_bert_model(train_data, test_data, save_dir, model_name='bert-base-uncased', epochs=3, batch_size=16):\n",
        "    \"\"\"\n",
        "    Fine-tunes a BERT model for text classification.\n",
        "\n",
        "    Args:\n",
        "        train_data (ReviewDataset): Training dataset.\n",
        "        test_data (ReviewDataset): Testing dataset.\n",
        "        save_dir (str): Directory to save the trained model.\n",
        "        model_name (str): Name of the pretrained BERT model.\n",
        "        epochs (int): Number of training epochs.\n",
        "        batch_size (int): Batch size for training.\n",
        "\n",
        "    Returns:\n",
        "        Trainer: The trained model.\n",
        "    \"\"\"\n",
        "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=save_dir,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='f1',\n",
        "        greater_is_better=True\n",
        "    )\n",
        "\n",
        "    def calculate_metrics(predictions):\n",
        "        true_labels = predictions.label_ids\n",
        "        predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "        acc = accuracy_score(true_labels, predicted_labels)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='binary')\n",
        "        return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=train_data.tokenizer)\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_data,\n",
        "        eval_dataset=test_data,\n",
        "        tokenizer=train_data.tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=calculate_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "R6mw4yrS5yfi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform LDA topic modeling\n",
        "def perform_lda_analysis(tokenized_docs, num_topics=10):\n",
        "    \"\"\"\n",
        "    Conducts LDA topic modeling on tokenized documents.\n",
        "\n",
        "    Args:\n",
        "        tokenized_docs (List[List[str]]): Preprocessed documents as token lists.\n",
        "        num_topics (int): Number of topics to extract.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[models.LdaModel, float]: LDA model and coherence score.\n",
        "    \"\"\"\n",
        "    vocab_dictionary = corpora.Dictionary(tokenized_docs)\n",
        "    document_corpus = [vocab_dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "    lda_model = models.LdaModel(document_corpus, num_topics=num_topics, id2word=vocab_dictionary, passes=10, random_state=42)\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=tokenized_docs, dictionary=vocab_dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    return lda_model, coherence_score\n"
      ],
      "metadata": {
        "id": "mBZjOM4b51Ba"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution pipeline\n",
        "def execute_pipeline():\n",
        "    imdb_file = 'data/IMDB_Dataset.csv'\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    print(\"Preparing IMDB dataset for training and testing...\")\n",
        "    train_data, test_data = prepare_review_data(imdb_file, tokenizer)\n",
        "\n",
        "    print(\"Fine-tuning the BERT model...\")\n",
        "    train_bert_model(train_data, test_data, save_dir='models/bert_sentiment_model')\n",
        "\n",
        "    print(\"Performing LDA topic modeling on example documents...\")\n",
        "    example_docs = [clean_text(\"Example document about machine learning.\") for _ in range(10)]\n",
        "    lda_model, coherence = perform_lda_analysis(example_docs)\n",
        "    print(f\"LDA Coherence Score: {coherence}\")\n"
      ],
      "metadata": {
        "id": "_r3GLTuF537k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Code\n",
        "def test_clean_text():\n",
        "    test_input = \"This is a TEST sentence, with punctuation!\"\n",
        "    expected_output = [\"test\", \"sentence\", \"punctuation\"]\n",
        "    assert clean_text(test_input) == expected_output, \"Text cleaning test failed.\"\n",
        "    print(\"Text cleaning test passed.\")\n",
        "\n",
        "def test_prepare_review_data():\n",
        "    test_data = pd.DataFrame({\n",
        "        'review': [\"Good movie!\", \"Bad movie!\"],\n",
        "        'sentiment': [\"positive\", \"negative\"]\n",
        "    })\n",
        "    test_data.to_csv(\"test_data.csv\", index=False)\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    train_data, test_data = prepare_review_data(\"test_data.csv\", tokenizer, split_ratio=0.5)\n",
        "    assert len(train_data) == 1 and len(test_data) == 1, \"Data preparation test failed.\"\n",
        "    print(\"Data preparation test passed.\")\n",
        "\n",
        "def test_perform_lda_analysis():\n",
        "    tokenized_docs = [[\"machine\", \"learning\", \"example\"], [\"artificial\", \"intelligence\"]]\n",
        "    lda_model, coherence = perform_lda_analysis(tokenized_docs, num_topics=2)\n",
        "    assert coherence > 0, \"LDA analysis test failed.\"\n",
        "    print(\"LDA analysis test passed.\")\n",
        "\n",
        "# Run tests\n",
        "def run_tests():\n",
        "    print(\"Running tests...\")\n",
        "    test_clean_text()\n",
        "    test_prepare_review_data()\n",
        "    test_perform_lda_analysis()\n",
        "    print(\"All tests passed.\")\n",
        "\n",
        "run_tests()\n"
      ],
      "metadata": {
        "id": "dES6AXYX56vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for consistent results\n",
        "def initialize_seed(seed_value=123):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "initialize_seed()\n",
        "\n",
        "# Text preprocessing function\n",
        "def clean_text(input_text):\n",
        "    \"\"\"\n",
        "    Cleans text by removing non-alphabet characters, converting to lowercase,\n",
        "    tokenizing, and filtering stopwords.\n",
        "\n",
        "    Args:\n",
        "        input_text (str): The text to process.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of processed words.\n",
        "    \"\"\"\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z]', ' ', input_text)\n",
        "    tokens = word_tokenize(cleaned_text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Dataset class for IMDB reviews\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, reviews, sentiments, tokenizer, max_len=512):\n",
        "        self.reviews = reviews\n",
        "        self.sentiments = sentiments\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        tokens = self.tokenizer(\n",
        "            self.reviews[index],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        tokens = {key: val.squeeze() for key, val in tokens.items()}\n",
        "        tokens['labels'] = torch.tensor(self.sentiments[index], dtype=torch.long)\n",
        "        return tokens\n",
        "\n",
        "# Function to load and split IMDB dataset\n",
        "def prepare_review_data(file_path, tokenizer, split_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Loads IMDB reviews and splits them into training and testing sets.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the dataset file.\n",
        "        tokenizer (BertTokenizer): Tokenizer for processing reviews.\n",
        "        split_ratio (float): Proportion of data for testing.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[ReviewDataset, ReviewDataset]: Training and testing datasets.\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(file_path)\n",
        "    sentiment_map = {'positive': 1, 'negative': 0}\n",
        "    data['sentiment_label'] = data['sentiment'].map(sentiment_map)\n",
        "    reviews = data['review'].tolist()\n",
        "    sentiments = data['sentiment_label'].tolist()\n",
        "\n",
        "    train_reviews, test_reviews, train_sentiments, test_sentiments = train_test_split(\n",
        "        reviews, sentiments, test_size=split_ratio, stratify=sentiments, random_state=42\n",
        "    )\n",
        "\n",
        "    train_data = ReviewDataset(train_reviews, train_sentiments, tokenizer)\n",
        "    test_data = ReviewDataset(test_reviews, test_sentiments, tokenizer)\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "# Function to fine-tune a BERT model\n",
        "def train_bert_model(train_data, test_data, save_dir, model_name='bert-base-uncased', epochs=3, batch_size=16):\n",
        "    \"\"\"\n",
        "    Fine-tunes a BERT model for text classification.\n",
        "\n",
        "    Args:\n",
        "        train_data (ReviewDataset): Training dataset.\n",
        "        test_data (ReviewDataset): Testing dataset.\n",
        "        save_dir (str): Directory to save the trained model.\n",
        "        model_name (str): Name of the pretrained BERT model.\n",
        "        epochs (int): Number of training epochs.\n",
        "        batch_size (int): Batch size for training.\n",
        "\n",
        "    Returns:\n",
        "        Trainer: The trained model.\n",
        "    \"\"\"\n",
        "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=save_dir,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='f1',\n",
        "        greater_is_better=True\n",
        "    )\n",
        "\n",
        "    def calculate_metrics(predictions):\n",
        "        true_labels = predictions.label_ids\n",
        "        predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "        acc = accuracy_score(true_labels, predicted_labels)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='binary')\n",
        "        return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=train_data.tokenizer)\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_data,\n",
        "        eval_dataset=test_data,\n",
        "        tokenizer=train_data.tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=calculate_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return trainer\n",
        "\n",
        "# Function to perform LDA topic modeling\n",
        "def perform_lda_analysis(tokenized_docs, num_topics=10):\n",
        "    \"\"\"\n",
        "    Conducts LDA topic modeling on tokenized documents.\n",
        "\n",
        "    Args:\n",
        "        tokenized_docs (List[List[str]]): Preprocessed documents as token lists.\n",
        "        num_topics (int): Number of topics to extract.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[models.LdaModel, float]: LDA model and coherence score.\n",
        "    \"\"\"\n",
        "    vocab_dictionary = corpora.Dictionary(tokenized_docs)\n",
        "    document_corpus = [vocab_dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "    lda_model = models.LdaModel(document_corpus, num_topics=num_topics, id2word=vocab_dictionary, passes=10, random_state=42)\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=tokenized_docs, dictionary=vocab_dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    return lda_model, coherence_score\n",
        "\n",
        "# Main execution pipeline\n",
        "def execute_pipeline():\n",
        "    imdb_file = 'data/IMDB_Dataset.csv'\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    print(\"Preparing IMDB dataset for training and testing...\")\n",
        "    train_data, test_data = prepare_review_data(imdb_file, tokenizer)\n",
        "\n",
        "    print(\"Fine-tuning the BERT model...\")\n",
        "    train_bert_model(train_data, test_data, save_dir='models/bert_sentiment_model')\n",
        "\n",
        "    print(\"Performing LDA topic modeling on example documents...\")\n",
        "    example_docs = [clean_text(\"Example document about machine learning.\") for _ in range(10)]\n",
        "    lda_model, coherence = perform_lda_analysis(example_docs)\n",
        "    print(f\"LDA Coherence Score: {coherence}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    execute_pipeline()\n"
      ],
      "metadata": {
        "id": "jWK1b-0T6-iV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}